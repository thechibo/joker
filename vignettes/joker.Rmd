---
title: "An Introduction to joker"
author: "Ioannis Oikonomidis"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{joker}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ">"
)
library(joker)
```

The `joker` package provides a comprehensive set of features for probabilities and mathematical statistics. It extends the range of available distribution families and facilitates the computation of key parametric quantities, such as moments and information-theoretic measures. The main focus of the package is parameter estimation through maximum likelihood and moment-based methods under an intuitive and efficient framework. 

## Introduction 

Package `joker` is designed to cover a broad collection of distribution families, extending the functionalities of the `stats` package to support new families, parametric quantity computation and parameter estimation. All package features are available both in a `stats`-like syntax for entry-level users, and in an `S4` object-oriented programming system for more experienced ones. This section introduces the state-of-the-art `R` packages in this direction, highlighting both the advantages and the shortcomings of each package.

## The joker `S4` Distribution System

### Probability Distributions

In the `joker` OOP system each distribution has a respective `S4` class, all of which are subclasses of the `Distribution` `S4` class. Defining an object from the desired distribution class is straightforward, as seen in the following example. The parameter names, which are generally identical to the ones defined in the `stats` package, can be omitted.

```{r}
shape1 <- 1 
shape2 <- 2
D <- Beta(shape1, shape2)
```

Having defined the distribution object `D`, the `d()`-`p()`-`q()`-`r()` functions can be used, as shown in the following example, comparing against the `stats` syntax.

```{r}
d(D, 0.5)
dbeta(0.5, shape1, shape2)

p(D, 0.5)
pbeta(0.5, shape1, shape2)

qn(D, 0.75)
qbeta(0.75, shape1, shape2)

r(D, 2)
rbeta(2, shape1, shape2)
```

Alternatively, if only the distribution argument is supplied, the methods behave as functionals (i.e. they return a function). This behavior offers enhanced functionality such as:

```{r}
F1 <- p(D)
F1(0.5)
```

*Technical Detail:* The quantile function is called `qn()` rather than the more intuitive `q()`. The reason behind this choice lies in the `RStudio` IDE (Integrated Development Environment), which overrides the method selection process of the `base` function `q()` used to quit an `R` session, i.e. a function named `q()` always ends the session. In order to avoid this unpleasant behavior, the name `qn()` was chosen instead.

### Parametric Quantities of Interest

The `joker` package contains a set of methods that calculate the theoretical moments (expectation, variance and standard deviation, skewness, excess kurtosis) and other important parametric functions (median, mode, entropy, Fisher information) of a distribution. Alternatively, the `moments()` function automatically finds the available methods for a given distribution and returns all of the results in a list.

```{r}
mean(D)
median(D)
mode(D)
var(D)
sd(D)
skew(D)
kurt(D)
entro(D)
finf(D)
```


*Technical Detail:* Only the function-distribution combinations that are theoretically defined are available; for example, while `var()` is available for all distributions, `sd()` is available only for the univariate ones. In case the result is not unique, a predetermined value is returned with a warning. The following example illustrates this in the case of $\mathcal{B}(1, 1)$, i.e. a uniform distribution for which every value in the $[0, 1]$ interval is a mode.

```{r}
mode(Beta(1, 1))
```

## Parameter Estimation

The `joker` package includes a number of options when it comes to parameter estimation. In order to illustrate these alternatives, a random sample is generated from the Beta distribution.

```{r}
set.seed(1)
shape1 <- 1
shape2 <- 2
D <- Beta(shape1, shape2)
x <- r(D, 100)
```

### Estimation Methods

The package covers three major estimation methods: maximum likelihood estimation (MLE), moment estimation (ME), and score-adjusted estimation (SAME).

In order to perform parameter estimation, a new `e<name>()` member is added to the `d()`-`p()`-`q()`-`r()` family, following the standard `stats` name convention. These `e<name>()` functions take two arguments, the observations `x` (an atomic vector for univariate or a matrix for multivariate distibutions) and the `type` of estimation method to use (a character with possible values `"mle"`, `"me"`, and `"same"`).

```{r}
ebeta(x, type = "mle")
ebeta(x, type = "me")
ebeta(x, type = "same")
```

Point estimation functions are available in two versions, the distribution specific one, e.g. `ebeta()`, and the `S4` generic ones, namely `mle()`, `me()`, and `same()`. A general function called `e()` is also implemented, covering all distributions and estimators.

```{r}
mle(D, x)
me(D, x)
same(D, x)
e(D, x, type = "mle")
```


*Technical Detail:* It is important to note that the `S4` methods also accept a character for the distribution. The name should be the same as the `S4` distribution generator, case ignored.

```{r, eval=FALSE}
mle("beta", x)
mle("bEtA", x)
e("Beta", x, type = "mle")
```

### Log-likelihood

Log-likelihood functions are also available in two versions, the distribution specific one, e.g. `llbeta()`, and the `ll()` `S4` generic one.

```{r}
llbeta(x, shape1, shape2)
ll(D, x)
```

In some distribution families like beta and gamma, the MLE cannot be explicitly derived and numerical optimization algorithms have to be employed. Even in *good* scenarios, with plenty of observations and a smooth optimization function, numerical algorithms should not be viewed as panacea, and extra care should be taken to ensure a fast and right convergence if possible. Two important steps are taken in `joker` in this direction:

\begin{enumerate}
  - The log-likelihood function is analytically calculated for each distribution family, so that constant terms with respect to the parameters can be removed, leaving only the sufficient statistics as a requirement for the function evaluation.
  - Multidimensional problems are reduced to unidimensional ones by utilizing the score equations.
\end{enumerate}

An illustrative example for the Beta distribution is shown below. Let $f$ denote the probability density function of $X\sim\mathcal{B}(\alpha,\beta)$:

\[
f(x; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}x^{\alpha-1} (1 - x)^{\beta-1}, \quad 0 < x < 1,
\]

where \( \Gamma \) is the Gamma function. Then, the log-likelihood function, divided by the sample size $n$, takes the form:

\[
\ell(\alpha, \beta) = (\alpha - 1) \overline{\log X} + (\beta - 1) \overline{\log (1 - X)} - \log \Gamma(\alpha) - \log \Gamma(\beta) + \log \Gamma(\alpha + \beta).
\]

The score equation for \( \alpha \) is:

\[
\frac{\partial \ell}{\partial \alpha}(\alpha, \beta) = \overline{\log X} - \psi(\alpha) + \psi(\alpha + \beta) = 0.
\]

The score equation for \( \beta \) is:

\[
\frac{\partial \ell}{\partial \beta}(\alpha, \beta) = \overline{\log (1 - X)} - \psi(\beta) + \psi(\alpha + \beta) = 0.
\]

These two nonlinear equations must be solved numerically. However, instead of solving the above two-dimensional problem, one can see that by denoting $c := \alpha + \beta$, the two score equations can be rewritten as:

\[
  \alpha = \psi^{-1}\left[\psi(c) + \overline{\log X}\right] \quad \beta = \psi^{-1}\left[\psi(c) + \overline{\log (1-X)}\right],
\]

i.e. restricted to the score equation system solution space, both parameters can be expressed as a function of their sum $c$, and therefore the log-likelihood function can be optimized with respect to $c$:

\[
\ell^\star(c) = \left[\alpha(c) - 1\right] \overline{\log X} + \left[\beta(c) - 1\right] \overline{\log (1 - X)} - \log \Gamma\left[\alpha(c)\right] - \log \Gamma\left[\beta(c)\right] + \log \Gamma(c).
\]

*Technical Detail:* It would perhaps be more intuitive to use the score equations to express $\alpha$ as a function of $\beta$ or vice versa. However, the above method can be directly generalized to the Dirichlet case and reduce the initial $k$-dimensional problem to a unidimensional one. The same technique can be utilized for the gamma and multivariate gamma distribution families, also reducing the dimension to unity, from $2$ and $k+1$ respectively.

In `joker`, the resulting function that is inserted in the optimization algorithm is called `lloptim()`, and is not to be confused with the actual log-likelihood function `ll()`. The corresponding derivative is called `dlloptim()`. Therefore, whenever numerical computation of the MLE is required, `joker` calls the `optim()` function with the following arguments:

- `lloptim()`, an efficient function to be optimized,
- `dlloptim()`, its analytically-computed derivate,
- the ME or SAME as the starting point (user's choice),
- the L-BFGS-U optimization algorithm, with lower and upper limits defined by default as the parameter space boundary.
  
### Asymptotic Variance - Covariance Matrix

The asymptotic variance (or variance - covariance matrix for multidimensional parameters) of the estimators are also covered in the package by the `v<name>()` functions. 

```{r}
vbeta(shape1, shape2, type = "mle")
vbeta(shape1, shape2, type = "me")
vbeta(shape1, shape2, type = "same")
```

As with point estimation, the implementation is twofold, and the general function `v()` covers all distributions and estimators.

```{r, eval=FALSE}
avar(D, type = "mle")
avar_mle(D)
avar_me(D)
avar_same(D)
```

## Estimation Metrics and Comparison

The different estimators of a parameter can be compared based on both finite sample and asymptotic properties. The package includes two functions named  `small_metrics()`  and  `large_metrics()` , where small and large refers to the *small sample* and *large sample* terms that are often used for the two cases. The former estimates the bias, variance and root mean square error (RMSE) of the estimator with Monte Carlo simulations, while the latter calculates the asymptotic variance - covariance matrix (as derived by the \code{avar} functions). The resulting data frames can be plotted with the `plot()` function.

To illustrate the function's design, consider the following example from the beta distribution: We are interested to calculate the metrics (bias, variance, and RMSE) of the $\alpha$ parameter estimators (MLE, ME, and SAME), for sample sizes 20 and 50. Specifically, we want to illustrate how these metrics change for $\alpha\in[1,5]$, and $\beta=2$ (constant). The following code can do that:

```{r}
D <- Beta(1, 2)

prm <- list(name = "shape1",
            val = seq(1, 5, by = 0.5))

x <- small_metrics(D, prm,
             obs = c(20, 50),
             est = c("mle", "same", "me"),
             sam = 1e2,
             seed = 1)

head(x@df)
```

The  `small_metrics()`  function takes the following arguments:

- `D`, the distribution object of interest,
- `prm`, a list that specifies how the `shape1` parameter values should change,
- `obs`, a numeric vector holding the sample sizes,
- `est`, a character vector specifying the estimators under comparison,
- `sam`, the Monte Carlo sample size to use for the metrics estimation,
- `seed`, a seed to be passed to `set.seed()` for replicability.

The resulting data frame can be passed to `plot()` to see the results. This `plot()` method depends on `ggplot2` to provide a highly-customizable graph.

```{r echo=FALSE, fig.height=8, fig.width=14, out.width="100%", fig.cap="Small-sample metrics comparison for MLE, ME, and SAME of the beta distribution $\alpha$ parameter."}
plot(x)
```

Note that in some distribution families the parameter is a vector, as is the case with the Dirichlet distribution (a multivariate generalization of beta), which holds a single parameter vector `alpha`. In these cases, the `prm` list can include a third element, `pos`, specifying which parameter of the vector should change:

```{r}
D <- Dir(alpha = 1:4)

prm <- list(name = "alpha",
            pos = 1,
            val = seq(1, 5, by = 0.5))

x <- small_metrics(D, prm,
                   obs = c(20, 50),
                   est = c("mle", "same", "me"),
                   sam = 1e2,
                   seed = 1)

class(x)
head(x@df)
```

The  `large_metrics()`  function design is almost identical, except that no `obs`, `sam`, and `seed` arguments are needed here. The following example illustrates the large sample metrics for the beta distribution shape $\alpha$ estimators. Again, the resulting data frame can be passed to `plot()`.

```{r}
D <- Beta(1, 2)

prm <- list(name = "shape1",
            val = seq(1, 5, by = 0.1))

x <- large_metrics(D, prm,
                   est = c("mle", "same", "me"))

class(x)
head(x@df)
```

```{r echo=FALSE, fig.height=8, fig.width=14, fig.cap="Large-sample metrics comparison for MLE, ME, and SAME of the beta distribution $\alpha$ parameter."}
plot(x)
```

## Documentation and Testing

### Documentation

### Testing

The R package is rigorously tested to ensure reliability, correctness, and stability. More than 1,000 automated tests have been implemented using the `testthat` package, a widely used framework for unit testing in R. These tests cover a broad range of functionalities, including edge cases, error handling, and performance checks, to verify that every function behaves as expected under various conditions. Continuous testing helps detect potential regressions early, maintaining the integrity of the package as it evolves. By leveraging `testthat`, we ensure that all updates and modifications uphold the expected behavior and performance standards.

## Defining New Classes and Methods

Of course, it is possible to be interested in a distribution family not included in the package. It is straightforward for users to define their own `S4` class and methods. Since this paper is addressed to both novice and experienced `R` users, the beta distribution paradigm is explained in detail below:

### Defining the Class

The `setClass()` function defines a new `S4` class, i.e. the distribution of interest. The `slots` argument defines the parameters and their respective class (usually numeric, but it can also be a matrix in distributions like the multivariate normal and the Wishart). The optional argument `prototype` can be used to define the default parameter values in case they are not specified by the user.

```{r, eval = FALSE}
setClass("Beta",
  contains = "Distribution",
  slots = c(shape1 = "numeric", shape2 = "numeric"),
  prototype = list(shape1 = 1, shape2 = 1))
```

### Defining a Generator

Now that the class is defined, one can type `D <- new("Beta", shape1 = shape1, shape2 = shape2)` to create a new object of class `Beta`. However, this is not so intuitive, and a wrapper function with the class name can be used instead. This function, often called a *generator*, can be used to simply code `D <- Beta(1, 2)` and define a new object from the $\mathcal{B}(1,2)$ distribution. The parameter slots can be accessed with the `@` sign, as shown above.

```{r, eval = FALSE}
Beta <- function(shape1 = 1, shape2 = 1) {
  new("Beta", shape1 = shape1, shape2 = shape2)
}

D <- Beta(1, 2)
D@shape1
D@shape2
```

### Defining Validity Checks

This step is optional but rather essential. So far, a user could type `D <- Beta(-1, 2)` without any errors, even though the beta parameters are defined in $\mathbb{R}_{+}$. To prevent such behaviors (that will probably end in bugs further down the road), the developer is advised to create a `setValidity()` function, including all the necessary restrictions posed by the parameter space.

```{r, eval = FALSE}
setValidity("Beta", function(object) {
  if(length(object@shape1) != 1) {
    stop("shape1 has to be a numeric of length 1")
  }
  if(object@shape1 <= 0) {
    stop("shape1 has to be positive")
  }
  if(length(object@shape2) != 1) {
    stop("shape2 has to be a numeric of length 1")
  }
  if(object@shape2 <= 0) {
    stop("shape2 has to be positive")
  }
  TRUE
})
```


### Defining the Class Methods

Now that everything is set, it is time to define methods for the new class. Creating functions and `S4` methods in `R` are two very similar processes, except the latter wraps the function in `setMethod()` and specifies a signature class, as shown above. The package source code can be used to easily define all methods of interest for the new distribution class.

```{r}
# probability density function
setMethod("d", signature = c(distr = "Beta", x = "numeric"),
          function(distr, x) {
            dbeta(x, shape1 = distr@shape1, shape2 = distr@shape2)
          })

# (theoretical) expectation
setMethod("mean",
          signature  = c(x = "Beta"),
          definition = function(x) {

  x@shape1 / (x@shape1 + x@shape2)

})

# moment estimator
setMethod("me",
          signature  = c(distr = "Beta", x = "numeric"),
          definition = function(distr, x) {

  m  <- mean(x)
  m2 <- mean(x ^ 2)
  d  <- (m - m2) / (m2 - m ^ 2)

  c(shape1 = d * m, shape2 = d * (1 - m))

})
```

